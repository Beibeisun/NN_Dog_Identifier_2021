{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "The purpose of this project is to build a dog-identifier, based on image data. It will be an L-Layer Deep Neural Network model following the steps below - \n",
    "1. Compress and convert raw image data (check)\n",
    "2. Initialise parameters (check)\n",
    "3. Forward propagation (check)\n",
    "4. Compute cost function (+ regularisation) (check)\n",
    "5. Backpropagation (+ regularisation) (check)\n",
    "6. Update parameters (check)\n",
    "7. Gradient checking (check)\n",
    "8. Prediction accuracy / precision / recall (check) + F1 Score (check)\n",
    "9. Display data and provide manual labelling (1 = Yes, it's a dog; 0 = No, it's not a dog)\n",
    "10. Plot iteration vs. cost (training) & cost (cv)\n",
    "11. Plot lambda vs. cost (training) & cost (cv); the skier shape\n",
    "12. Plot m vs. cost (training) & cost (cv) to tell Bias vs. Variance\n",
    "13. Plot iteration vs. cost (traininng) for different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "from scipy import ndimage\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_param(layers_size):\n",
    "\n",
    "    L = len(layers_size)\n",
    "    parameters={}\n",
    "\n",
    "    for l in range (1,L):\n",
    "        parameters['W'+str(l)]=np.random.randn(layers_size[l],layers_size[l-1])*np.sqrt(2/layers_size[l-1])\n",
    "        parameters['b'+str(l)]=np.zeros((layers_size[l],1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    A=1/(1+np.exp(-Z))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \n",
    "    if Z<0:\n",
    "        A=0\n",
    "    \n",
    "    elif Z>=0:\n",
    "        A=Z\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_forward_prop(A_prev,W,b,activation):\n",
    "    \n",
    "    Z=np.dot(W,A_prev)+b\n",
    "    linear_cache= (A_prev,W,b)\n",
    "    activation_cache=Z\n",
    "    \n",
    "    if activation==\"tanh\":\n",
    "        A=np.tanh(Z) # Provided by numpy\n",
    "    \n",
    "    elif activation==\"sigmoid\":\n",
    "        A=sigmoid(Z) # Provided above\n",
    "        \n",
    "    elif activation==\"relu\":\n",
    "        A=relu(Z) # Provided above\n",
    "        \n",
    "    cache = (linear_cache,activation_cache)\n",
    "    \n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL,Y):\n",
    "    # Unregularised\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -(1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL),axis=1,keepdims=True)\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_backward_prop(dA,cache,activation,lambd):\n",
    "    \n",
    "    grads={}\n",
    "    linear_cache,activation_cache=cache\n",
    "    A_prev,W,b=linear_cache\n",
    "    Z=activation_cache\n",
    "    m=dA.shape[1]\n",
    "    \n",
    "    if activation==\"tanh\":\n",
    "        dZ=dA*(1-np.power(np.tanh(Z),2))\n",
    "    \n",
    "    elif activation==\"sigmoid\":\n",
    "        dZ=dA*sigmoid(Z)*(1-sigmoid(Z))\n",
    "    \n",
    "    elif activation==\"relu\":\n",
    "        if Z<=0:\n",
    "            dZ=0\n",
    "        \n",
    "        elif Z>0:\n",
    "            dZ=dA\n",
    "    \n",
    "    dW=(1/m)*np.dot(dZ,A_prev.T)+(lambd/m)*W\n",
    "    db=(1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev=np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_param(parameters,grads,learning_rate):\n",
    "    \n",
    "    L=len(parameters)//2\n",
    "    \n",
    "    for l in range (L):\n",
    "        parameters['W'+str(l+1)]=parameters['W'+str(l+1)]-learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters['b'+str(l+1)]=parameters['b'+str(l+1)]-learning_rate*grads['db'+str(l+1)]    \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dog_identifier(X,Y,layers_size,iters,learning_rate,lambd):\n",
    "    \n",
    "    # 1. Initialise parameters based on layers_size, L includes input layer\n",
    "    parameters=initialise_param(layers_size)\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # 2. FP with \"tanh\" except the last(output) layer,in total L-2 sets of caches\n",
    "    L = len(layers_size)\n",
    "    \n",
    "    for i in range(0,iters):\n",
    "        A_prev=X\n",
    "        caches=[]\n",
    "        regularisation = 0\n",
    "\n",
    "        for l in range (1,L-1):\n",
    "            W=parameters['W'+str(l)]\n",
    "            b=parameters['b'+str(l)]\n",
    "            activation=\"tanh\"\n",
    "            A,cache=single_forward_prop(A_prev,W,b,activation)\n",
    "            A_prev=A\n",
    "            caches.append(cache)\n",
    "            regularisation = regularisation + np.sum(np.square(W))\n",
    "\n",
    "        # 3. FP in the last layer and compute cost in layer L, with \"sigmoid\", now caches has L-1 layers\n",
    "        W=parameters['W'+str(L-1)]\n",
    "        b=parameters['b'+str(L-1)]\n",
    "        activation=\"sigmoid\"\n",
    "        A,cache=single_forward_prop(A_prev,W,b,activation)\n",
    "        AL=A\n",
    "        caches.append(cache)\n",
    "        regularisation = (lambd/(2*m))*(regularisation +np.sum(np.square(W)))\n",
    "        \n",
    "        cost=compute_cost(AL,Y)\n",
    "        cost = cost + regularisation\n",
    "\n",
    "        if (i+1)%50==0:\n",
    "            print(\"Iteration #\"+str(i+1)+\" Cost is \"+str(cost))\n",
    "\n",
    "        # 4. BP first with \"sigmoid\" for the last(output) layer\n",
    "        grads={}\n",
    "        dA = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
    "        cache = caches[L-2] # caches has L-1 layers, and the index for the last one is L-2\n",
    "        activation = \"sigmoid\"\n",
    "        # In grads, dA (previous) is always 1 layer prior to the dW and db\n",
    "        grads['dA'+str(L-2)],grads['dW'+str(L-1)],grads['db'+str(L-1)]=single_backward_prop(dA,cache,activation,lambd)\n",
    "\n",
    "        # 5. BP with L-2 layers of \"tanh\", going from l = L-3 to l = 0\n",
    "        for l in reversed(range(L-2)):\n",
    "            activation=\"tanh\"\n",
    "            dA_prev_temp,dW_temp,db_temp=single_backward_prop(grads['dA'+str(l+1)],caches[l],activation,lambd)\n",
    "            grads['dA'+str(l)]=dA_prev_temp\n",
    "            grads['dW'+str(l+1)]=dW_temp\n",
    "            grads['db'+str(l+1)]=db_temp    \n",
    "\n",
    "        #6. update parameters with GD\n",
    "        parameters=update_param(parameters,grads,learning_rate)\n",
    "    \n",
    "    return parameters, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters):\n",
    "    \n",
    "    L=len(parameters)//2 # This L does NOT include input layer\n",
    "    A_prev=X\n",
    "    \n",
    "    for l in range(L-1):\n",
    "        W=parameters['W'+str(l+1)]\n",
    "        b=parameters['b'+str(l+1)]\n",
    "        activation=\"tanh\"\n",
    "        A,cache=single_forward_prop(A_prev,W,b,activation)\n",
    "        A_prev=A\n",
    "    \n",
    "    W=parameters['W'+str(L)]\n",
    "    b=parameters['b'+str(L)]\n",
    "    activation=\"sigmoid\"\n",
    "    A,cache=single_forward_prop(A_prev,W,b,activation)\n",
    "    AL=A\n",
    "    \n",
    "    m=AL.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    for i in range(m):\n",
    "        \n",
    "        if AL[0,i]>0.5:\n",
    "            p[0,i]=1\n",
    "        \n",
    "        else:\n",
    "            p[0,i]=0\n",
    "            \n",
    "    precision = np.sum(p*Y==1)/np.sum(p==1)\n",
    "    recall = np.sum(p*Y==1)/np.sum(Y==1)\n",
    "    f1 = (2*precision*recall)/(precision+recall)\n",
    "    print('Accuracy is '+str(np.sum(p==Y)/m))\n",
    "    print('Precision is '+str(precision))\n",
    "    print('Recall is '+str(recall))\n",
    "    print('F1 Score is '+str(f1))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data set X and Y, can be for training, CV or test\n",
    "def create_data_set(arr_dog,arr_nondog,num_px,pathpos,pathneg):\n",
    "    \n",
    "    mdog=len(arr_dog)\n",
    "    mnondog=len(arr_nondog)\n",
    "    X=np.zeros((num_px*num_px*3,mdog+mnondog))\n",
    "    Y=np.zeros((1,mdog+mnondog))\n",
    "    \n",
    "    for i in range(mdog):\n",
    "        im=arr_dog[i]\n",
    "        fname=pathpos+im\n",
    "        temp=Image.open(fname)\n",
    "        temp=temp.resize((num_px,num_px))\n",
    "        temp=np.array(temp)\n",
    "        im_flatten=temp.ravel().T\n",
    "        im_flatten=im_flatten/255\n",
    "        X[:,i]=im_flatten\n",
    "        Y[0,i]=1\n",
    "    \n",
    "    for i in range(mnondog):\n",
    "        im=arr_nondog[i]\n",
    "        fname=pathneg+im\n",
    "        temp=Image.open(fname)\n",
    "        temp=temp.resize((num_px,num_px))\n",
    "        temp=np.array(temp)\n",
    "        im_flatten=temp.ravel().T\n",
    "        im_flatten=im_flatten/255\n",
    "        X[:,mdog+i]=im_flatten\n",
    "        Y[0,mdog+i]=0\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(parameters,grads,training_X,training_Y, epsilon):\n",
    "    \n",
    "    param_array = dictionary_to_vector(parameters)\n",
    "    num_param = param_array.shape[0] # Number of parameters in every W and b\n",
    "    \n",
    "    gradientapprox=np.zeros((num_param,1))\n",
    "    \n",
    "    cost_plus = np.zeros((num_param,1))\n",
    "    cost_minus = np.zeros((num_param,1))\n",
    "    \n",
    "    param_plus = {}\n",
    "    param_minus = {}\n",
    "    \n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    # Unroll the gradient without epsilon\n",
    "    gradient = gradients_to_vector(grads)\n",
    "    \n",
    "    # Update the parameters to plus epsilon\n",
    "    for i in range(num_param):\n",
    "        # Convert list to array, update one parameter value, then convert back to array\n",
    "        param_plus_temp = np.copy(param_array)\n",
    "        param_plus_temp[i][0]=param_plus_temp[i][0]+epsilon        \n",
    "        param_plus=vector_to_dictionary(param_plus_temp,parameters)\n",
    "        \n",
    "        # FP with updated list up to second last layer\n",
    "        A_prev=training_X\n",
    "\n",
    "        for l in range (1,L):\n",
    "            W=param_plus['W'+str(l)]\n",
    "            b=param_plus['b'+str(l)]\n",
    "            activation=\"tanh\"\n",
    "            A,cache=single_forward_prop(A_prev,W,b,activation)\n",
    "            A_prev=A\n",
    "\n",
    "        # FP in the last layer and compute cost in layer L, with \"sigmoid\"\n",
    "        W=param_plus['W'+str(L)]\n",
    "        b=param_plus['b'+str(L)]\n",
    "        activation=\"sigmoid\"\n",
    "        A,cache=single_forward_prop(A_prev,W,b,activation)\n",
    "        AL=A\n",
    "\n",
    "        cost_plus[i]=compute_cost(AL,training_Y)    \n",
    "    \n",
    "    # Update the parameters to minus epsilon\n",
    "    for i in range(num_param):\n",
    "        # Convert list to array, update one parameter value, then convert back to array\n",
    "        param_minus_temp = np.copy(param_array)\n",
    "        param_minus_temp[i][0]=param_minus_temp[i][0]-epsilon        \n",
    "        param_minus=vector_to_dictionary(param_minus_temp,parameters)\n",
    "        \n",
    "        # FP with updated list up to second last layer\n",
    "        A_prev=training_X\n",
    "\n",
    "        for l in range (1,L):\n",
    "            W=param_minus['W'+str(l)]\n",
    "            b=param_minus['b'+str(l)]\n",
    "            activation=\"tanh\"\n",
    "            A,cache=single_forward_prop(A_prev,W,b,activation)\n",
    "            A_prev=A\n",
    "\n",
    "        # FP in the last layer and compute cost in layer L, with \"sigmoid\"\n",
    "        W=param_minus['W'+str(L)]\n",
    "        b=param_minus['b'+str(L)]\n",
    "        activation=\"sigmoid\"\n",
    "        A,cache=single_forward_prop(A_prev,W,b,activation)\n",
    "        AL=A\n",
    "\n",
    "        cost_minus[i]=compute_cost(AL,training_Y)       \n",
    "    \n",
    "    # Calculate grandientapprox\n",
    "    gradientapprox = (cost_plus-cost_minus)/(2*epsilon)\n",
    "    \n",
    "    numerator = np.linalg.norm(gradient-gradientapprox)\n",
    "    denominator = np.linalg.norm(gradient)+np.linalg.norm(gradientapprox)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_to_vector(parameters):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    count = 0\n",
    "    \n",
    "    L=len(parameters)//2\n",
    "    param = {}\n",
    "    \n",
    "    for i in range(L):\n",
    "        param['W'+str(i+1)]=parameters['W'+str(i+1)]\n",
    "        param['b'+str(i+1)]=parameters['b'+str(i+1)]\n",
    "    \n",
    "    for key in (param):\n",
    "        \n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(param[key], (-1,1))\n",
    "        keys = keys + [key]*new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients_to_vector(grads):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    count = 0\n",
    "    \n",
    "    L=len(grads)//3\n",
    "    gradients = {}\n",
    "    \n",
    "    for i in range(L):\n",
    "        gradients['dW'+str(i+1)]=grads['dW'+str(i+1)]\n",
    "        gradients['db'+str(i+1)]=grads['db'+str(i+1)]\n",
    "    \n",
    "    for key in (gradients):\n",
    "        \n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(gradients[key], (-1,1))\n",
    "        keys = keys + [key]*new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_dictionary(theta,parameters):\n",
    "\n",
    "    param = {}\n",
    "    L=len(parameters)//2\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(L):\n",
    "        temp1=parameters['W'+str(i+1)]\n",
    "        temp2=parameters['b'+str(i+1)]\n",
    "        \n",
    "        param['W'+str(i+1)] = theta[count:(count+(temp1.shape[1]*temp1.shape[0]))].reshape(temp1.shape)\n",
    "        count = count + temp1.shape[1]*temp1.shape[0]\n",
    "        \n",
    "        param['b'+str(i+1)] = theta[count:(count+temp2.shape[1]*temp2.shape[0])].reshape(temp2.shape)\n",
    "        count = count + temp2.shape[1]*temp2.shape[0]\n",
    "    \n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #50 Cost is 0.995350387157117\n",
      "Iteration #100 Cost is 0.9674521892796787\n",
      "Iteration #150 Cost is 0.9345632345605823\n",
      "Iteration #200 Cost is 0.9049390534879251\n",
      "Iteration #250 Cost is 0.8771895618067906\n",
      "Iteration #300 Cost is 0.8497841333348384\n",
      "Iteration #350 Cost is 0.8222831560931603\n",
      "Iteration #400 Cost is 0.7945889897167555\n",
      "Iteration #450 Cost is 0.7666778105182295\n",
      "Iteration #500 Cost is 0.7878661842052821\n",
      "Iteration #550 Cost is 0.7685038301047675\n",
      "Iteration #600 Cost is 0.7498928810931559\n",
      "Iteration #650 Cost is 0.7315665832673495\n",
      "Iteration #700 Cost is 0.7133306026410593\n",
      "Iteration #750 Cost is 0.6950160332832697\n",
      "Iteration #800 Cost is 0.6765791193292212\n",
      "107\n",
      "Accuracy is 0.897196261682243\n",
      "Precision is 0.8472222222222222\n",
      "Recall is 1.0\n",
      "F1 Score is 0.9172932330827067\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1.]]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Use this command box to launch the training set and get parameters\n",
    "pathpos='images/Dog/'\n",
    "pathneg='images/Non-Dog/'\n",
    "arr_dog = os.listdir(pathpos)\n",
    "arr_nondog=os.listdir(pathneg)\n",
    "num_px=32\n",
    "X,Y=create_data_set(arr_dog,arr_nondog,num_px,pathpos,pathneg)\n",
    "training_X=X\n",
    "training_Y=Y\n",
    "layers_size=[num_px*num_px*3,20,10,5,1]\n",
    "learning_rate=0.003\n",
    "iters=800\n",
    "lambd=1\n",
    "parameters, grads=dog_identifier(training_X,training_Y,layers_size,iters,learning_rate,lambd)\n",
    "print(training_X.shape[1])\n",
    "p=predict(training_X, training_Y, parameters)\n",
    "print(p)\n",
    "print(len(grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.5614035087719298\n",
      "Precision is 0.4594594594594595\n",
      "Recall is 0.7727272727272727\n",
      "F1 Score is 0.576271186440678\n",
      "[[1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1.\n",
      "  0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      "  1. 1. 1. 1. 1. 1. 0. 1. 1.]]\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "# Use this command box to create test sets\n",
    "pathpos='images/Test-Dog/'\n",
    "pathneg='images/Test-Non-Dog/'\n",
    "arr_dog = os.listdir(pathpos)\n",
    "arr_nondog=os.listdir(pathneg)\n",
    "X,Y=create_data_set(arr_dog,arr_nondog,num_px,pathpos,pathneg)\n",
    "test_X=X\n",
    "test_Y=Y\n",
    "p=predict(test_X, test_Y, parameters)\n",
    "print(p)\n",
    "print(test_X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7\n",
    "difference = gradient_checking(parameters,grads,training_X,training_Y, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00040681595968665977\n"
     ]
    }
   ],
   "source": [
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
